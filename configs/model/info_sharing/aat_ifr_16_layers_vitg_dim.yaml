# Model type (Options: ["cross_attention", "global_attention", "alternating_attention"])
model_type: "alternating_attention"
# Model class type (Options: ["no_intermediate_features", "intermediate_features"])
model_return_type: "intermediate_features"
# Custom positional encoding (Options: ["RoPEfreq"], Callable Function, null)
custom_positional_encoding: null
# Module arguments
module_args:
  # Name of the info sharing module
  name: "aat_16_layers_vitg_dim_ifr"
  # Indices of the intermediate features to be shared (indices start from 0)
  indices: [7, 11]
  # Normalize intermediate features
  norm_intermediate: True
  # Size string
  size: "16_layers"
  # Depth (this includes both frame-wise and gloabl attention layers)
  depth: 16
  # Distinguish Reference and Non-Reference Views
  distinguish_ref_and_non_ref_views: True
  # Flag to indicate whether to use gradient checkpointing
  gradient_checkpointing: False
  # Feature dim (similar to ViT-Giant)
  dim: 1536
  # Number of heads (similar to ViT-Giant)
  num_heads: 24
  # Set transformer parameters similar to DINOv2
  mlp_ratio: 4
  qkv_bias: True
  qk_norm: False
  init_values: 1e-5
